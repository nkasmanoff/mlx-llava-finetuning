{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 199728.76it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from generate import load_model\n",
    "from tuner.utils import linear_to_lora_layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from mlx.utils import tree_flatten\n",
    "import mlx.nn as nn\n",
    "import mlx.core as mx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor, model = load_model(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceM4/ScienceQAImg_Modif\")\n",
    "max_tokens, temperature = 128, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_trainable_parameters(model):\n",
    "    def nparams(m):\n",
    "        if isinstance(m, nn.QuantizedLinear):\n",
    "            return m.weight.size * (32 // m.bits)\n",
    "        return sum(v.size for _, v in tree_flatten(m.parameters()))\n",
    "\n",
    "    leaf_modules = tree_flatten(\n",
    "        model.leaf_modules(), is_leaf=lambda m: isinstance(m, nn.Module)\n",
    "    )\n",
    "    total_p = sum(nparams(m) for _, m in leaf_modules) / 10**6\n",
    "    trainable_p = (\n",
    "        sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "    )\n",
    "    print(\n",
    "        f\"Trainable parameters: {(trainable_p * 100 / total_p):.3f}% \"\n",
    "        f\"({trainable_p:.3f}M/{total_p:.3f}M)\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 0.011% (0.786M/7063.426M)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.freeze()\n",
    "lora_layers = 4\n",
    "lora_parameters = {\n",
    "    \"keys\": [\"self_attn.q_proj\", \"self_attn.v_proj\", \"self_attn.k_proj\", \"self_attn.out_proj\"],\n",
    "    \"rank\": 8,\n",
    "    \"alpha\": 16.0,\n",
    "    \"scale\": 10.0,\n",
    "    \"dropout\": 0.0,\n",
    "}\n",
    "\n",
    "linear_to_lora_layers(model.language_model.model, lora_layers, lora_parameters)\n",
    "\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import load_image\n",
    "\n",
    "label_dict = {0:'A',\n",
    " 1:'B',\n",
    " 2:'C',\n",
    " 3:'D',\n",
    " 4:'E'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "idx = 0\n",
    "image = dataset['train'][idx]['image']\n",
    "context = dataset['train'][idx]['context']\n",
    "answer = label_dict[dataset['train'][idx]['label']]\n",
    "\n",
    "\n",
    "\n",
    "def prepare_inputs(processor, image, prompt):\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "    inputs = processor(prompt, image, return_tensors=\"np\", padding=True)\n",
    "\n",
    "    pixel_values = mx.array(inputs[\"pixel_values\"])\n",
    "    input_ids = mx.array(inputs[\"input_ids\"])\n",
    "    return input_ids, pixel_values\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def default_loss(inputs, targets):\n",
    "\n",
    "    ntoks = targets.shape[0]\n",
    "    ce = nn.losses.cross_entropy(inputs, targets)\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce\n",
    "\n",
    "def full_loss(model, inputs):\n",
    "    input_ids, pixel_values, targets = inputs\n",
    "    image_positions = [np.where(input_id == model.config.image_token_index)[0][0] for input_id in input_ids]\n",
    "\n",
    "    logits, _ = model(input_ids, pixel_values)\n",
    "    logits = logits.astype(mx.float32)\n",
    "    output_size = logits.shape[1]\n",
    "\n",
    "\n",
    "    shift_size = output_size - input_ids.shape[1]\n",
    "\n",
    "\n",
    "    output_logits = [\n",
    "        logits[i, int(image_positions[i] + shift_size  ):-1] for i in range(BATCH_SIZE)\n",
    "        ]\n",
    "\n",
    "    ce = [default_loss(output_logits[i], targets[i]) for i in range(BATCH_SIZE)]\n",
    "    return mx.stack(ce).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/6218 [00:05<8:43:09,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.022279598712921143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 101/6218 [03:02<2:51:10,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4978646662831308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 201/6218 [05:58<2:43:22,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.176883435845375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 301/6218 [08:55<2:47:26,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0417833179235458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 401/6218 [11:49<2:34:22,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9420208588242531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 404/6218 [11:56<2:51:56,  1.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 46\u001b[0m\n\u001b[1;32m     40\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (input_ids, pixel_values, targets)\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m step(inputs)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "image_token_index = model.config.image_token_index\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "learning_rate = 1e-5\n",
    "model.train()\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss_value_and_grad = nn.value_and_grad(model, full_loss)\n",
    "\n",
    "def step(inputs):\n",
    "    # Forward and backward pass\n",
    "    lvalue, grad = loss_value_and_grad(model, inputs)\n",
    "\n",
    "    # Model update\n",
    "    opt.update(model, grad)\n",
    "\n",
    "    return lvalue\n",
    "\n",
    "state = [model.state, opt.state]\n",
    "avg_loss = 0.0\n",
    "for epoch in range(1):\n",
    "    for i in tqdm(range(0, len(dataset[\"train\"]), BATCH_SIZE)):\n",
    "\n",
    "\n",
    "        batch_images =  [dataset['train'][i]['image'] for i in range(i, i+BATCH_SIZE)]\n",
    "        batch_contexts = [dataset['train'][i]['context'] for i in range(i, i+BATCH_SIZE)]\n",
    "        batch_answers = [label_dict[dataset['train'][i]['label']] for i in range(i, i+BATCH_SIZE)]\n",
    "\n",
    "        batch_prompts = []\n",
    "        for j in range(len(batch_images)):\n",
    "            batch_prompts.append(f\"USER:\\n<image>\\n{batch_contexts[j]}\\nASSISTANT:\\n{batch_answers[j]}</s>\")\n",
    "\n",
    "        input_ids, pixel_values = prepare_inputs(processor, batch_images, batch_prompts)\n",
    "\n",
    "        image_positions = [np.where(input_id == image_token_index)[0][0] for input_id in input_ids]\n",
    "        targets = [input_id[int(image_positions[i] +1 ):] for i, input_id in enumerate(input_ids)]\n",
    "\n",
    "        input_size = input_ids.shape[1]\n",
    "\n",
    "        inputs = (input_ids, pixel_values, targets)\n",
    "\n",
    "        \n",
    "        loss = step(inputs)\n",
    "\n",
    "\n",
    "        mx.eval(state, loss)\n",
    "        avg_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Loss: {avg_loss / 100}\")\n",
    "            avg_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_adapter(\n",
    "    model: nn.Module,\n",
    "    adapter_file: str,\n",
    "):\n",
    "    flattened_tree = tree_flatten(model.trainable_parameters())\n",
    "\n",
    "    mx.savez(adapter_file, **dict(flattened_tree))\n",
    "\n",
    "\n",
    "checkpoint_adapter_file = \"adapter.npz\"\n",
    "save_adapter(model=model, adapter_file=checkpoint_adapter_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the adapter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 226719.14it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "processor, model = load_model(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "# Same LoRA parameters as before\n",
    "model.freeze()\n",
    "lora_layers = 4\n",
    "lora_parameters = {\n",
    "    \"keys\": [\"self_attn.q_proj\", \"self_attn.v_proj\", \"self_attn.k_proj\", \"self_attn.out_proj\"],\n",
    "    \"rank\": 8,\n",
    "    \"alpha\": 16.0,\n",
    "    \"scale\": 10.0,\n",
    "    \"dropout\": 0.0,\n",
    "}\n",
    "\n",
    "linear_to_lora_layers(model.language_model.model, lora_layers, lora_parameters)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_adapter_file = \"adapter.npz\"\n",
    "model.load_weights(checkpoint_adapter_file, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"USER:\\n<image>\\n{context}\\nASSISTANT:\\n\"\n",
    "prompt = f\"{prompt}\"[:256]\n",
    "input_ids, pixel_values = prepare_inputs(processor, image, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each direction.\n",
      "Question: Which of the following is the capital of Oklahoma?\n",
      "Choices:\n",
      "A. Oklahoma City\n",
      "B. Tulsa\n",
      "C. Norman\n",
      "D. Oklahoma\n",
      "Answer with the letter.\n",
      "ASSISTANT:\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "from generate import generate_text\n",
    "model.eval()\n",
    "temperature=.01\n",
    "reply = generate_text(input_ids, pixel_values, model, processor, 512, temperature)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:\n",
      "<image>\n",
      "Lecture: Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\n",
      "A compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
